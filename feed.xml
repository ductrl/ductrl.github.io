<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ductrl.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ductrl.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-11T09:37:39+00:00</updated><id>https://ductrl.github.io/feed.xml</id><title type="html">Mike Ly</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://ductrl.github.io/blog/2025/2025-06-05-predict-nba-position-using-k-means-clustering/" rel="alternate" type="text/html" title=""/><published>2025-06-11T09:37:39+00:00</published><updated>2025-06-11T09:37:39+00:00</updated><id>https://ductrl.github.io/blog/2025/2025-06-05-predict-nba-position-using-k-means-clustering</id><content type="html" xml:base="https://ductrl.github.io/blog/2025/2025-06-05-predict-nba-position-using-k-means-clustering/"><![CDATA[<p>In this project, we will use K-means clustering to <em>sort of</em> predict an NBA player’s position based on their height and weight. For more detailed descriptions and codes, check out the <a href="https://colab.research.google.com/drive/1CCqjMUi7J4fvKlMkVkInOEJ0ZqlXIgVt?usp=sharing">Google Colab notebook</a> of this project to learn more.</p> <p><br/></p> <h1 id="1-project-setup">1. Project Setup</h1> <p>Based on NBA players’ heights and weights, we will use <strong>K-means clustering</strong> to determine whether players can be grouped into position-based clusters. (k = 3 for guard, forward, center; k = 5 for PG, SG, SF, PF, C).</p> <p><br/></p> <h1 id="2-data-preparation">2. Data Preparation</h1> <p>For this project, we will use the <a href="https://www.kaggle.com/datasets/damirdizdarevic/nba-dataset-eda-and-ml-compatible/data">NBA Player Data (1996-2024)</a> dataset from Kaggle. This dataset provides the height and weight of individual NBA players (as well as other stats, which we will not use in this project.). As there is no label, an unsupervised learning algorithm such as K-means clustering can come in handy.</p> <p><br/></p> <h3 id="data-cleaning">Data cleaning</h3> <p>Firstly, it might be useful to visualize the raw data as a scatterplot. We will do this again after our model has clustered the data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">player_height</span><span class="sh">'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">player_weight</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Height (cm)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Weight (kg)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <img src="https://i.postimg.cc/kXKdsQQT/image.png" alt="my alt text"/> <figcaption style="text-align: center; font-size: 0.9em; color: #888; margin-top: 0.5em;"> The generated scatterplot. </figcaption> </figure> <p>From this plot, we can see that the clusters aren’t visually obvious, and there are possible outliers (players below 170 cm).</p> <blockquote class="block-warning"> <h5 id="warning">WARNING</h5> <p>As I am writing this blog post, I realized that I did not address the outliers at all. One way you can deal with this is simply by removing them (some common thresholds are samples that are at least 3 standard deviations from the means or the IQR rule). Another way is to treat them like missing data and impute them.</p> </blockquote> <p>This dataset does not have any empty values, so this is not an issue. However, since height and weight have different scales, we want to rescale them so that one feature doesn’t weigh (no pun intended) more than the other. In this case, I prefer standardization to normalization because K-means clustering uses Euclidean distance, which is sensitive to variance, and standardization preserves the variance of the data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">scaled_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
</code></pre></div></div> <p>Now, using <code class="language-plaintext highlighter-rouge">scaled_df.describe()</code>, we should see that the scaled data has a mean of 0 and a standard deviation of 1.</p> <p><br/></p> <h1 id="3-modeling">3. Modeling</h1> <p>Implementing a K-means clustering model is quite straightforward. Here, we will use <code class="language-plaintext highlighter-rouge">KMeans</code> from <code class="language-plaintext highlighter-rouge">sklearn.cluster</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">scaled_df</span><span class="p">)</span>
</code></pre></div></div> <p>Now we can visualize the clusters labeled by our model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">player_height</span><span class="sh">'</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">player_weight</span><span class="sh">'</span><span class="p">],</span> <span class="n">hue</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">labels_</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Height (cm)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Weight (kg)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <img src="https://i.postimg.cc/Y04tMTm7/image.png" alt="my alt text"/> <figcaption style="text-align: center; font-size: 0.9em; color: #888; margin-top: 0.5em;"> The result scatterplot. </figcaption> </figure> <p>Based on typical height and weight ranges, we can infer that cluster 0 likely represents guards, 1 represents forwards, and 2 represents centers. You can try and see if these results would correctly categorize your favorite NBA player’s position or not! Just like with the linear regression model, this model also does not do well with Giannis. Maybe he’s just a one-of-a-kind player :)</p> <p><br/></p> <h1 id="4-final-words">4. Final Words</h1> <p>While you’re here, this is part 2 of a machine learning project series where I apply every machine learning algorithm I learned to an NBA-related project. If you want to check out more similar projects, look around my blog and stay tuned for more!</p>]]></content><author><name></name></author></entry><entry><title type="html">Predicting NBA Player Salary Using Linear Regression</title><link href="https://ductrl.github.io/blog/2025/predicting-nba-player-salary-using-linear-regression/" rel="alternate" type="text/html" title="Predicting NBA Player Salary Using Linear Regression"/><published>2025-06-04T15:00:00+00:00</published><updated>2025-06-04T15:00:00+00:00</updated><id>https://ductrl.github.io/blog/2025/predicting-nba-player-salary-using-linear-regression</id><content type="html" xml:base="https://ductrl.github.io/blog/2025/predicting-nba-player-salary-using-linear-regression/"><![CDATA[<p>In this project, we will try to predict an NBA player’s salary just from their regular season stats by using a linear regression model. You can have a look at this project’s <a href="https://colab.research.google.com/drive/1OPzUx9T_YxQ4eBxVRwVIsJiUjN7TDRo8?usp=sharing">Google Colab notebook</a> for more details.</p> <h1 id="1-project-setup">1. Project Setup</h1> <p>Using various individual performance stats, we will predict an NBA player’s salary with a linear regression model.</p> <p><br/></p> <h1 id="2-data-preparation">2. Data Preparation</h1> <p>For this project, we will use data set from Kaggle called <a href="https://www.kaggle.com/datasets/loganlauton/nba-players-and-team-data?select=NBA+Player+Stats%281950+-+2022%29.csv">NBA Players &amp; Team Data</a>, which provides multiple datasets, including one for player individual stats and another one for salaries. Since they are separate datasets, we will clean them individually and then merge them.</p> <p><br/></p> <h3 id="data-cleaning">Data cleaning</h3> <p>By taking a look at <code class="language-plaintext highlighter-rouge">df_stats.columns</code>, we can see that the dataset provides a lot of potentially useful data. However, to keep the input simple enough so that a user can play around with it, the only categories we will use are Age, PPG, APG, RPG, SPG, BPG. Moreover, as player valuation based on stats varies significantly across eras, I will not use data from before 2018, thus ensuring relevancy.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">relevant_columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Season</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Player</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">G</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">AST</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">TRB</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">STL</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">BLK</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">PTS</span><span class="sh">'</span><span class="p">]</span>
<span class="n">df_stats</span> <span class="o">=</span> <span class="n">df_stats</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="n">items</span><span class="o">=</span><span class="n">relevant_columns</span><span class="p">)</span>
<span class="n">df_stats</span> <span class="o">=</span> <span class="n">df_stats</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">Season</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">2018</span><span class="p">]</span>
</code></pre></div></div> <blockquote class="block-tip"> <h5 id="tip">TIP</h5> <p>Rather than removing all data before 2018, another alternative is to use the inflation-adjusted salary provided by the dataset. A good reason to do this is that you will end up with much more data than the 1658 I ended up with.</p> </blockquote> <p>Since the salary dataset only provides data up until 2021, unlike the stats dataset which goes up to 2024, we will first handle mismatching data and empty rows, and then merge them. After that, we can drop all the empty rows (which means dropping data from 2022 to 2024).</p> <p>The first issue we need to address is that the two datasets have name formatting inconsistencies in special cases: 1) Players with special characters in their names, e.g., D.J. Augustin; 2) Players with name prefixes, e.g., Marvin Bagley III. There are other special cases like JJ Barea, where the salary dataset uses the full unabbreviated name. These cases are too complex to resolve programmatically as they would require manual detection and fixing, so we will just drop them. The naming inconsistencies are addressed with this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># removing suffixes
</span><span class="n">suffixes</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">III</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">II</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">IV</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">V</span><span class="sh">'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">suffix</span> <span class="ow">in</span> <span class="n">suffixes</span><span class="p">:</span>
  <span class="n">df_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">Player</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">Player</span><span class="sh">'</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="n">suffix</span><span class="p">,</span> <span class="sh">''</span><span class="p">).</span><span class="nb">str</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span>
<span class="c1"># removing punctuations
</span><span class="n">df_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">Player</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">Player</span><span class="sh">'</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">).</span><span class="nb">str</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span>
<span class="n">df_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">Player</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">Player</span><span class="sh">'</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="se">\'</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">).</span><span class="nb">str</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span>
</code></pre></div></div> <p>Inspecting the salary dataset, we can see that many players do not have a salary value. This is because they are either on a two-way contract or a 10-day contract, or they are a G-League call-up. While we can impute these cases with, for instance, the minimum salary, I will just go with dropping them.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">dropna</span><span class="p">()</span>
</code></pre></div></div> <p>Like many monetary datasets, the salaries are also formatted in strings, e.g., <em>$24,157,304</em>. This can be addressed pretty easily:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">salary</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">salary</span><span class="sh">'</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">$</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">).</span><span class="nb">str</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div> <p><br/></p> <h3 id="feature-engineering">Feature engineering</h3> <p>The stats dataset does not provide averages in a season, but rather their totals. Thus, we will calculate the average-per-game statistics.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">total_stats</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">AST</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">TRB</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">STL</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">BLK</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">PTS</span><span class="sh">'</span><span class="p">]</span>
<span class="n">average_stats</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">APG</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">RPG</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SPG</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">BPG</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">PPG</span><span class="sh">'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range </span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  <span class="n">df</span><span class="p">[</span><span class="n">average_stats</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">total_stats</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">G</span><span class="sh">'</span><span class="p">]</span>
  <span class="n">df</span><span class="p">[</span><span class="n">average_stats</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">average_stats</span><span class="p">[</span><span class="n">i</span><span class="p">]].</span><span class="nf">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div> <p><br/></p> <h3 id="data-splitting">Data splitting</h3> <p>We will simply go with the classic 80% train and 20% test split using <code class="language-plaintext highlighter-rouge">sklearn</code>’s <code class="language-plaintext highlighter-rouge">train_test_split</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
</code></pre></div></div> <p><br/></p> <h1 id="3-modeling">3. Modeling</h1> <p>Finally, the good part! As advertised, we will use a linear regression model to predict the salary.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div> <p>We will evaluate the model using the Mean Absolute Error and R-squared score.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">r2_score</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mae</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="n">r2</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">On average, the model is off by $</span><span class="si">{</span><span class="n">mae</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">R-squared: </span><span class="si">{</span><span class="n">r2</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>From the MAE score, we learn that the model is off by about $4.4M on average. Honestly, this is not too bad since I did not expect a linear relationship between player stats and their salaries.</p> <p>And there we have it! You can try out the model yourself to see how much money it expects your favorite player to make. An example of how to do so:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">giannis</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
    <span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">30</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">APG</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">6.5</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">RPG</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">SPG</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">BPG</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.2</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">PPG</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">30.4</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">giannis_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">giannis</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1000000</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Giannis Antetokounmpo makes $</span><span class="si">{</span><span class="n">giannis_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">M</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>After playing around with it, I found that the model works pretty great on role players. Try modifying the input stats or adding new features to see how the predictions change and play around with the model yourself!</p> <p><br/></p> <h1 id="4-final-words">4. Final Words</h1> <p>While you’re here, this is part 1 of a machine learning project series where I apply every machine learning algorithm I learned to an NBA-related project. If you want to check out more similar projects, look around my blog and stay tune for more!</p>]]></content><author><name></name></author><category term="machine-learning-projects"/><category term="machine-learning"/><category term="nba"/><summary type="html"><![CDATA[Predict NBA salaries from their stats using a linear regression model]]></summary></entry><entry><title type="html">The most underrated human ability</title><link href="https://ductrl.github.io/blog/2024/the-most-underrated-human-ability/" rel="alternate" type="text/html" title="The most underrated human ability"/><published>2024-06-24T12:02:32+00:00</published><updated>2024-06-24T12:02:32+00:00</updated><id>https://ductrl.github.io/blog/2024/the-most-underrated-human-ability</id><content type="html" xml:base="https://ductrl.github.io/blog/2024/the-most-underrated-human-ability/"><![CDATA[<p>While sitting in a convenience store waiting for the heavy rain to stop, a friend of mine randomly asked one of the most classic questions in our group chat: <em>‘If you can have one superpower, what would that be?’</em> Though I didn’t answer, I approached the question a little differently: <strong>What is humanity’s most important ability?</strong></p> <p>I was curious. Compared to other animals, aren’t humans already <em>superanimals</em>? Thus, I find it amusing to look at our super abilities and spend a moment admiring them.</p> <p>Well, what’s Google’s answer? Critical thinking, communication, adaptability, creativity, and the list goes on. As I uncovered more answers from the web, there was one ability that I couldn’t find anywhere — the ability to <strong>memorize</strong>.</p> <p>It is so astounding to me, not just because of the fact that it was an answer that I couldn’t find anywhere, but also to think about the world where no one has the ability to memorize. Imagine it with me: A world where everyone would be a stranger to each other, where you can never recognize a familiar face, where no knowledge will ever be passed on, where you will never be able to learn something new; even worse, what if you forget to breathe? What if you forget to eat? What if you forget the feeling of thirst, and never knew that you need to drink a cup of water before the world before your eyes just becomes dark?</p> <p>Humans develop by building on top of existing knowledge, knowledge that was discovered by humans themselves. When taking the picture of a world with no memory to the extreme, humanity crumbles, a fascinating picture regardless. That fascination, when reflected in the real world, grows exponentially when you look at those with memories robbed — those with dementia.</p> <p>When many (mostly unintentionally) found individuals with dementia to be quite annoying to talk to — understandably so as you’re talking to someone who might forget the conversation in seconds, to someone who might be agitated easily, and many more inconveniences — I find these people to be one of the most interesting to converse with. How can you not be curious, when you can learn how they cherish the memories that they retain, and the way they value such memories compared to us? When the whole world is a stranger, how does that affect your worldview? When your memory becomes a blank canvas, how does your creativity differ? When the past becomes an unknown, how do you approach the future? When life takes away your ability to memorize, what is the one memory that you wish to remember till the end of your life?</p> <p>I have yet to have a chance to ask these questions to people with dementia, but when I say I do research about the disorder, I find it untruthful to say that my purpose is not only to somehow help them, but also to learn from them, to access the world from a novel view that I have yet to be able to. In a reality where every human is so unique that it is nearly impossible to look at life from every distinct perspective, wouldn’t it be cool to look at the same thing every day with a refreshed mind, to look at the world from the view of those who share the <em>memory of nothingness</em>?</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9ce1517446fa" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>